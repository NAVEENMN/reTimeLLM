{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TimeLLM\n",
    "\n",
    "The paper introduces **TIME-LLM**, a framework that reprograms large language models (LLMs) for time series forecasting without modifying their pre-trained backbones. It transforms time series data into text-like formats using \"text prototypes\" and leverages a novel **Prompt-as-Prefix (PaP)** technique to guide LLMs. TIME-LLM achieves superior performance over specialized forecasting models in both few-shot and zero-shot scenarios. This approach bridges the gap between time series and language data, showcasing LLMs' potential to generalize across diverse tasks. Comprehensive evaluations demonstrate significant accuracy and efficiency improvements. The framework points towards using multimodal foundation models for time series forecasting.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e99a4ce94b496f86"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Problem Formulation\n",
    "\n",
    "Given a sequence of historical observations $X \\in R^{N \\times T}$, where $T$ is time steps and $N$ is 1 dimensional variables.\n",
    "\n",
    "The framework aims to reprogram a LLM $f(.)$ To accurately forecast the $H$ future time steps denoted by $\\hat{Y} \\in R^{N \\times H}$ by minimizing Objective function\n",
    "\n",
    "$min(\\frac{1}{H} \\left( \\sum_{h=1}^{H} \\left\\| \\hat{Y}_h - Y_h \\right\\|^2_F \\right))$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d338f937f7425b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Methodology\n",
    "Initially, a multivariate time series is partitioned into $N$ univariate time series, which are subsequently processed independently (Nie et al., 2023). The i-th series is denoted as $X^{(i)} \\in R^{1 \\times T}$ , which undergoes normalization, patching, and embedding prior to being reprogrammed with learned text prototypes to align the source and target modalities. Then, we augment the LLMâ€™s time series reasoning ability by prompting it together with reprogrammed patches to generate output representations, which are projected to the final forecasts $\\hat{Y}^{(i)} \\in R^{1 \\times H}$. We note that only the parameters of the lightweight input transformation and output projection are updated, while the backbone language model is frozen. In contrast to vision-language and other multimodal language models, which usually fine-tune with paired cross-modality data, TIME-LLM is directly optimized and becomes readily available with only a small set of time series and a few training epochs, maintaining high efficiency and imposing fewer resource constraints compared to building large domain-specific models from scratch or fine-tuning them. To further reduce memory footprints, various off-the-shelf techniques (e.g., quantization) can be seamlessly integrated for slimming TIME-LLM. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb9b8d820a29a9a"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0               1               2               3      \\\n",
      "date  2016/7/1 00:00  2016/7/1 01:00  2016/7/1 02:00  2016/7/1 03:00   \n",
      "HUFL           5.827           5.693           5.157            5.09   \n",
      "HULL           2.009           2.076           1.741           1.942   \n",
      "MUFL           1.599           1.492           1.279           1.279   \n",
      "MULL           0.462           0.426           0.355           0.391   \n",
      "\n",
      "               4               5               6               7      \\\n",
      "date  2016/7/1 04:00  2016/7/1 05:00  2016/7/1 06:00  2016/7/1 07:00   \n",
      "HUFL           5.358           5.626           7.167           7.435   \n",
      "HULL           1.942           2.143           2.947           3.282   \n",
      "MUFL           1.492           1.528           2.132            2.31   \n",
      "MULL           0.462           0.533           0.782           1.031   \n",
      "\n",
      "               8               9      ...            17410            17411  \\\n",
      "date  2016/7/1 08:00  2016/7/1 09:00  ...  2018/6/26 10:00  2018/6/26 11:00   \n",
      "HUFL           5.559           4.555  ...           -6.497          -10.449   \n",
      "HULL           3.014           2.545  ...            6.162             5.09   \n",
      "MUFL           2.452           1.919  ...          -10.483          -14.463   \n",
      "MULL           1.173           0.817  ...            3.767            3.198   \n",
      "\n",
      "                17412            17413            17414            17415  \\\n",
      "date  2018/6/26 12:00  2018/6/26 13:00  2018/6/26 14:00  2018/6/26 15:00   \n",
      "HUFL          -15.271           -6.229           -1.273           -1.674   \n",
      "HULL            3.818            3.483            3.617             3.55   \n",
      "MUFL       -17.554001           -9.666           -4.904           -5.615   \n",
      "MULL            2.452            1.528            2.132            2.132   \n",
      "\n",
      "                17416            17417            17418            17419  \n",
      "date  2018/6/26 16:00  2018/6/26 17:00  2018/6/26 18:00  2018/6/26 19:00  \n",
      "HUFL           -5.492            2.813            9.243           10.114  \n",
      "HULL            4.287            3.818            3.818             3.55  \n",
      "MUFL           -9.132           -0.817            5.472            6.183  \n",
      "MULL            2.274            2.097            2.097            1.564  \n",
      "\n",
      "[5 rows x 17420 columns]\n",
      "shape: (8, 17420)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_raw = pd.read_csv(\"data/ETTh1.csv\")\n",
    "df_raw = df_raw.T\n",
    "print(df_raw.head())\n",
    "print(f\"shape: {df_raw.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-11T19:14:25.409975Z",
     "start_time": "2024-10-11T19:14:25.200325Z"
    }
   },
   "id": "a741f08966e07b0e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look at how these data is created into patches as part of processing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3a0dd5ef06216332"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  5.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1:\n",
      "\n",
      "batch x: This is the input sequence for the model, containing the historical time series data points that the model will use to predict future values.\n",
      "batch x: torch.Size([24, 384, 1])\n",
      "\n",
      "batch y: This is the target sequence for the model, containing the true future values that the model will try to predict.\n",
      "batch y: torch.Size([24, 192, 1])\n",
      "\n",
      "batch x': This is the time encoding for the input sequence, which provides additional information about the time of day, day of the week, etc.\n",
      "batch x': torch.Size([24, 384, 4])\n",
      "\n",
      "batch y': This is the time encoding for the target sequence, which provides additional information about the time of day, day of the week, etc.\n",
      "batch y': torch.Size([24, 192, 4])\n",
      "\n",
      "\n",
      "batch 2:\n",
      "\n",
      "batch x: This is the input sequence for the model, containing the historical time series data points that the model will use to predict future values.\n",
      "batch x: torch.Size([24, 384, 1])\n",
      "\n",
      "batch y: This is the target sequence for the model, containing the true future values that the model will try to predict.\n",
      "batch y: torch.Size([24, 192, 1])\n",
      "\n",
      "batch x': This is the time encoding for the input sequence, which provides additional information about the time of day, day of the week, etc.\n",
      "batch x': torch.Size([24, 384, 4])\n",
      "\n",
      "batch y': This is the time encoding for the target sequence, which provides additional information about the time of day, day of the week, etc.\n",
      "batch y': torch.Size([24, 192, 4])\n",
      "\n",
      "\n",
      "batch 3:\n",
      "\n",
      "batch x: This is the input sequence for the model, containing the historical time series data points that the model will use to predict future values.\n",
      "batch x: torch.Size([24, 384, 1])\n",
      "\n",
      "batch y: This is the target sequence for the model, containing the true future values that the model will try to predict.\n",
      "batch y: torch.Size([24, 192, 1])\n",
      "\n",
      "batch x': This is the time encoding for the input sequence, which provides additional information about the time of day, day of the week, etc.\n",
      "batch x': torch.Size([24, 384, 4])\n",
      "\n",
      "batch y': This is the time encoding for the target sequence, which provides additional information about the time of day, day of the week, etc.\n",
      "batch y': torch.Size([24, 192, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dataloader import Dataset_ETT_hour\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset = Dataset_ETT_hour()\n",
    "data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=24,\n",
    "        shuffle=False,\n",
    "        num_workers=1,\n",
    "        drop_last=True\n",
    "    )\n",
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in tqdm(enumerate(data_loader)):\n",
    "        print(f\"batch {i + 1}:\")\n",
    "        print(\"\\nbatch x: This is the input sequence for the model, containing the historical time series data points that the model will use to predict future values.\")\n",
    "        print(\"batch x:\", batch_x.shape)\n",
    "        print(\"\\nbatch y: This is the target sequence for the model, containing the true future values that the model will try to predict.\")\n",
    "        print(\"batch y:\", batch_y.shape)\n",
    "        print(\"\\nbatch x': This is the time encoding for the input sequence, which provides additional information about the time of day, day of the week, etc.\")\n",
    "        print(\"batch x':\", batch_x_mark.shape)\n",
    "        print(\"\\nbatch y': This is the time encoding for the target sequence, which provides additional information about the time of day, day of the week, etc.\")\n",
    "        print(\"batch y':\", batch_y_mark.shape)\n",
    "        print(\"\\n\")\n",
    "\n",
    "        if i == 2:\n",
    "            break\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T22:02:23.219712Z",
     "start_time": "2024-10-10T22:02:16.794292Z"
    }
   },
   "id": "594c763f8d0089bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's build the model step by step."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4cbb0b47a4f2349e"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from layers.StandardNorm import Normalize\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, configs, patch_len=16, stride=8):\n",
    "        super(Model, self).__init__()\n",
    "        self.task_name = \"long_term_forecast\"\n",
    "        self.pred_len = 96\n",
    "        self.seq_len = 512\n",
    "        self.d_ff = 128\n",
    "        self.top_k = 5\n",
    "        self.d_llm = 4096\n",
    "        self.patch_len = 16\n",
    "        self.stride = 8\n",
    "        \n",
    "        # taken as it is from the paper\n",
    "        # configs.enc_in = 7\n",
    "        self.normalize_layers = Normalize(7, affine=False)\n",
    "    \n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n",
    "        dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
    "        return dec_out\n",
    "        #return dec_out[:, -self.pred_len:, :]\n",
    "    \n",
    "    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "            x_enc = self.normalize_layers(x_enc, 'norm')\n",
    "            return x_enc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T22:02:23.231658Z",
     "start_time": "2024-10-10T22:02:23.228651Z"
    }
   },
   "id": "a6d29152cbe17190"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n",
      "tensor([[30.5310, 27.7870, 27.7870, 25.0440, 21.9480, 21.1740, 22.7920, 23.1440,\n",
      "         21.6670, 17.4460, 19.9790, 20.1190, 19.2050, 18.5720, 19.5560, 17.3050,\n",
      "         19.4860, 19.1340, 20.6820, 18.7120, 17.8680, 18.0090, 18.0090, 19.7680,\n",
      "         21.1040, 19.6970, 20.0490, 20.7520, 21.3850, 22.2300, 20.2600, 21.1040,\n",
      "         20.6120, 18.3610, 20.9630, 19.4160, 20.8230, 20.1900, 21.3150, 22.0190,\n",
      "         20.6820, 25.4660, 25.8880, 27.8570, 27.2950, 22.2300, 21.9480, 27.2950,\n",
      "         29.3350, 26.0280, 24.3400, 26.4500, 25.9580, 24.0590, 25.3250, 23.6370,\n",
      "         26.3800, 27.3650, 28.0680, 29.4750, 26.8020, 29.9680, 30.3900, 31.1640,\n",
      "         29.7570, 32.2890, 31.9380, 28.5610, 21.5260, 22.2300, 19.4160, 18.5720,\n",
      "         21.6670, 25.5360, 27.8570, 27.9280, 24.6210, 23.8480, 23.0740, 22.5110,\n",
      "         21.6670, 25.3950, 25.1840, 29.5460, 29.4750, 29.2640, 30.9530, 31.7260,\n",
      "         33.1330, 28.9830, 28.9830, 31.7260, 25.1840, 30.5310, 27.6460, 25.4660,\n",
      "         25.9580, 25.9580, 26.0280, 28.9130, 30.4600, 31.3750, 32.7820, 31.7260,\n",
      "         30.3200, 30.4600, 32.9220, 31.0930, 25.6770, 27.1540, 28.7720, 30.6010,\n",
      "         32.0080, 33.1330, 31.5150, 31.0230, 31.4450, 31.7260, 30.9530, 30.4600,\n",
      "         28.9130, 30.3900, 31.5150, 31.2340, 30.5310, 30.1090, 30.3200, 29.6160,\n",
      "         30.5310, 32.9930, 30.2490, 30.2490, 30.6710, 31.5860, 31.9380, 28.9830,\n",
      "         27.7170, 30.6710, 24.3400, 29.6860, 30.2490, 29.9680, 29.5460, 28.4910,\n",
      "         28.5610, 28.7720, 27.7170, 28.1390, 27.3650, 26.7320, 26.3800, 25.3950,\n",
      "         27.9280, 28.9830, 29.7570, 30.6010, 32.6410, 35.3850, 37.2840, 38.4800,\n",
      "         36.7910, 34.1180, 33.8370, 33.2740, 32.9220, 33.6260, 32.6410, 31.5860,\n",
      "         31.0930, 30.7420, 29.3350, 28.7720, 29.1240, 29.2640, 29.4050, 28.9130,\n",
      "         29.8970, 30.5310, 30.6710, 30.6710, 31.0930, 30.8820, 32.7110, 34.7510,\n",
      "         34.4000, 34.3290, 33.6260, 33.5560, 32.7820, 32.1490, 31.2340, 30.5310,\n",
      "         30.9530, 30.3200, 29.4050, 28.7020, 28.7720, 28.9130, 28.2790, 28.6310,\n",
      "         28.7720, 29.9680, 29.7570, 30.3200, 31.4450, 30.7420, 31.6560, 32.2890,\n",
      "         32.8520, 32.0780, 31.8670, 31.3750, 30.8820, 30.8820, 29.9680, 29.9680,\n",
      "         29.4750, 29.3350, 29.8270, 29.1240, 28.5610, 28.6310, 27.7870, 27.5060,\n",
      "         28.0680, 29.6160, 29.4750, 29.0530, 29.4750, 29.1240, 29.6860, 31.0230,\n",
      "         31.1640, 30.6710, 29.9680, 30.1790, 29.4750, 29.4050, 28.2090, 29.1240,\n",
      "         28.5610, 29.1940, 28.5610, 29.2640, 29.0530, 28.9130, 29.8270, 29.8270,\n",
      "         29.1940, 30.8820, 31.2340, 32.2190, 32.9930, 33.9070, 34.7510, 34.7510,\n",
      "         34.8220, 34.8920, 33.3440, 31.6560, 33.4850, 32.6410, 32.5000, 33.0630,\n",
      "         33.4850, 33.6960, 33.5560, 33.3440, 32.9930, 33.3440, 33.9070, 33.2040,\n",
      "         34.1890, 35.9470, 36.3690, 36.0880, 36.0880, 36.2290, 38.4090, 39.6050,\n",
      "         40.0980, 40.5200, 37.8470, 37.3540, 36.0180, 36.1580, 36.0180, 36.2290,\n",
      "         36.5100, 36.7210, 36.5800, 37.7060, 37.7060, 37.6360, 37.0030, 38.5500,\n",
      "         38.0580, 37.0030, 37.0030, 29.8270, 32.8520, 36.1580, 35.8070, 37.7760,\n",
      "         39.5350, 38.2690, 34.7510, 33.7670, 32.8520, 33.1330, 34.3290, 35.3850,\n",
      "         34.5400, 32.7110, 33.9070, 34.6810, 35.6660, 34.4700, 35.3140, 33.8370,\n",
      "         35.8770, 35.5250, 36.5800, 37.3540, 34.6110, 33.8370, 33.2040, 34.6810,\n",
      "         36.7210, 34.7510, 24.9030, 32.8520, 33.1330, 29.9680, 30.3900, 31.3750,\n",
      "         31.3040, 32.6410, 33.3440, 33.3440, 34.6110, 33.9070, 34.5400, 35.3850,\n",
      "         34.3290, 31.5860, 31.3750, 30.7420, 31.5860, 31.3750, 31.0230, 25.8170,\n",
      "         24.9030, 18.0090, 22.4410, 16.8830, 22.0890, 24.1990, 25.7470, 26.5210,\n",
      "         26.9430, 27.3650, 28.3500, 28.0680, 28.3500, 27.9280, 27.7170, 29.8970,\n",
      "         30.1790, 31.4450, 33.2040, 33.4150, 32.7110, 32.8520, 33.2040, 34.0480,\n",
      "         35.5250, 34.4700, 35.5960, 33.6260, 32.1490, 32.0080, 31.7260, 30.8820]],\n",
      "       dtype=torch.float64)\n",
      "torch.Size([24, 384, 1])\n",
      "tensor([[ 0.1481, -0.4135, -0.4135, -0.9749, -1.6085, -1.7669, -1.4358, -1.3637,\n",
      "         -1.6660, -2.5298, -2.0115, -1.9828, -2.1699, -2.2994, -2.0980, -2.5587,\n",
      "         -2.1124, -2.1844, -1.8676, -2.2708, -2.4435, -2.4146, -2.4146, -2.0546,\n",
      "         -1.7812, -2.0692, -1.9971, -1.8533, -1.7237, -1.5508, -1.9539, -1.7812,\n",
      "         -1.8819, -2.3426, -1.8101, -2.1267, -1.8387, -1.9683, -1.7380, -1.5940,\n",
      "         -1.8676, -0.8885, -0.8021, -0.3992, -0.5142, -1.5508, -1.6085, -0.5142,\n",
      "         -0.0967, -0.7735, -1.1190, -0.6871, -0.7878, -1.1765, -0.9174, -1.2628,\n",
      "         -0.7015, -0.4999, -0.3560, -0.0681, -0.6151,  0.0328,  0.1192,  0.2776,\n",
      "         -0.0103,  0.5078,  0.4360, -0.2551, -1.6949, -1.5508, -2.1267, -2.2994,\n",
      "         -1.6660, -0.8742, -0.3992, -0.3847, -1.0614, -1.2196, -1.3780, -1.4933,\n",
      "         -1.6660, -0.9030, -0.9462, -0.0535, -0.0681, -0.1112,  0.2344,  0.3926,\n",
      "          0.6806, -0.1687, -0.1687,  0.3926, -0.9462,  0.1481, -0.4424, -0.8885,\n",
      "         -0.7878, -0.7878, -0.7735, -0.1831,  0.1335,  0.3208,  0.6087,  0.3926,\n",
      "          0.1049,  0.1335,  0.6374,  0.2631, -0.8453, -0.5431, -0.2119,  0.1624,\n",
      "          0.4503,  0.6806,  0.3494,  0.2488,  0.3351,  0.3926,  0.2344,  0.1335,\n",
      "         -0.1831,  0.1192,  0.3494,  0.2919,  0.1481,  0.0617,  0.1049, -0.0392,\n",
      "          0.1481,  0.6519,  0.0904,  0.0904,  0.1767,  0.3640,  0.4360, -0.1687,\n",
      "         -0.4278,  0.1767, -1.1190, -0.0249,  0.0904,  0.0328, -0.0535, -0.2694,\n",
      "         -0.2551, -0.2119, -0.4278, -0.3415, -0.4999, -0.6294, -0.7015, -0.9030,\n",
      "         -0.3847, -0.1687, -0.0103,  0.1624,  0.5799,  1.1415,  1.5301,  1.7749,\n",
      "          1.4292,  0.8822,  0.8247,  0.7094,  0.6374,  0.7815,  0.5799,  0.3640,\n",
      "          0.2631,  0.1912, -0.0967, -0.2119, -0.1399, -0.1112, -0.0824, -0.1831,\n",
      "          0.0183,  0.1481,  0.1767,  0.1767,  0.2631,  0.2199,  0.5942,  1.0117,\n",
      "          0.9399,  0.9253,  0.7815,  0.7671,  0.6087,  0.4792,  0.2919,  0.1481,\n",
      "          0.2344,  0.1049, -0.0824, -0.2262, -0.2119, -0.1831, -0.3128, -0.2408,\n",
      "         -0.2119,  0.0328, -0.0103,  0.1049,  0.3351,  0.1912,  0.3783,  0.5078,\n",
      "          0.6231,  0.4647,  0.4215,  0.3208,  0.2199,  0.2199,  0.0328,  0.0328,\n",
      "         -0.0681, -0.0967,  0.0040, -0.1399, -0.2551, -0.2408, -0.4135, -0.4710,\n",
      "         -0.3560, -0.0392, -0.0681, -0.1544, -0.0681, -0.1399, -0.0249,  0.2488,\n",
      "          0.2776,  0.1767,  0.0328,  0.0760, -0.0681, -0.0824, -0.3271, -0.1399,\n",
      "         -0.2551, -0.1256, -0.2551, -0.1112, -0.1544, -0.1831,  0.0040,  0.0040,\n",
      "         -0.1256,  0.2199,  0.2919,  0.4935,  0.6519,  0.8390,  1.0117,  1.0117,\n",
      "          1.0262,  1.0406,  0.7238,  0.3783,  0.7526,  0.5799,  0.5510,  0.6663,\n",
      "          0.7526,  0.7958,  0.7671,  0.7238,  0.6519,  0.7238,  0.8390,  0.6951,\n",
      "          0.8967,  1.2565,  1.3428,  1.2853,  1.2853,  1.3142,  1.7603,  2.0051,\n",
      "          2.1060,  2.1924,  1.6453,  1.5444,  1.2710,  1.2997,  1.2710,  1.3142,\n",
      "          1.3717,  1.4149,  1.3860,  1.6165,  1.6165,  1.6021,  1.4726,  1.7892,\n",
      "          1.6885,  1.4726,  1.4726,  0.0040,  0.6231,  1.2997,  1.2278,  1.6308,\n",
      "          1.9908,  1.7317,  1.0117,  0.8103,  0.6231,  0.6806,  0.9253,  1.1415,\n",
      "          0.9685,  0.5942,  0.8390,  0.9974,  1.1990,  0.9542,  1.1269,  0.8247,\n",
      "          1.2421,  1.1701,  1.3860,  1.5444,  0.9831,  0.8247,  0.6951,  0.9974,\n",
      "          1.4149,  1.0117, -1.0037,  0.6231,  0.6806,  0.0328,  0.1192,  0.3208,\n",
      "          0.3063,  0.5799,  0.7238,  0.7238,  0.9831,  0.8390,  0.9685,  1.1415,\n",
      "          0.9253,  0.3640,  0.3208,  0.1912,  0.3640,  0.3208,  0.2488, -0.8167,\n",
      "         -1.0037, -2.4146, -1.5076, -2.6451, -1.5796, -1.1478, -0.8310, -0.6726,\n",
      "         -0.5862, -0.4999, -0.2983, -0.3560, -0.2983, -0.3847, -0.4278,  0.0183,\n",
      "          0.0760,  0.3351,  0.6951,  0.7383,  0.5942,  0.6231,  0.6951,  0.8678,\n",
      "          1.1701,  0.9542,  1.1846,  0.7815,  0.4792,  0.4503,  0.3926,  0.2199]],\n",
      "       dtype=torch.float64)\n",
      "torch.Size([24, 384, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset_ETT_hour()\n",
    "data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=24,\n",
    "        shuffle=False,\n",
    "        num_workers=1,\n",
    "        drop_last=True\n",
    "    )\n",
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in tqdm(enumerate(data_loader)):\n",
    "        print(f\"batch {i}\")\n",
    "        model = Model(configs=None)\n",
    "        print(torch.reshape(batch_x[0], (1, 384)))\n",
    "        print(batch_x.shape)\n",
    "        output = model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n",
    "        print(torch.reshape(output[0], (1, 384)))\n",
    "        print(output.shape)\n",
    "        if i == 0:\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T22:02:29.269503Z",
     "start_time": "2024-10-10T22:02:23.234982Z"
    }
   },
   "id": "3baa8afadc373a2b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Prompt generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aac30e4e9350c7d6"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "def generate_prompt(x_enc, description, pred_len, seq_len):\n",
    "    def calcute_lags(_x_enc):\n",
    "        q_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)\n",
    "        k_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)\n",
    "        res = q_fft * torch.conj(k_fft)\n",
    "        corr = torch.fft.irfft(res, dim=-1)\n",
    "        mean_value = torch.mean(corr, dim=1)\n",
    "        _, lags = torch.topk(mean_value, 5, dim=-1)\n",
    "        return lags\n",
    "    \n",
    "    B, T, N = x_enc.size()\n",
    "    x_enc = x_enc.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n",
    "    min_values = torch.min(x_enc, dim=1)[0]\n",
    "    max_values = torch.max(x_enc, dim=1)[0]\n",
    "    medians = torch.median(x_enc, dim=1).values\n",
    "    lags = calcute_lags(x_enc)\n",
    "    trends = x_enc.diff(dim=1).sum(dim=1)\n",
    "    \n",
    "    prompt = []\n",
    "    for b in range(x_enc.shape[0]):\n",
    "        min_values_str = str(min_values[b].tolist()[0])\n",
    "        max_values_str = str(max_values[b].tolist()[0])\n",
    "        median_values_str = str(medians[b].tolist()[0])\n",
    "        lags_values_str = str(lags[b].tolist())\n",
    "        prompt_ = (\n",
    "            f\"<|start_prompt|>Dataset description: {description}\"\n",
    "            f\"Task description: forecast the next {str(pred_len)} steps given the previous {str(seq_len)} steps information; \"\n",
    "            \"Input statistics: \"\n",
    "            f\"min value {min_values_str}, \"\n",
    "            f\"max value {max_values_str}, \"\n",
    "            f\"median value {median_values_str}, \"\n",
    "            f\"the trend of input is {'upward' if trends[b] > 0 else 'downward'}, \"\n",
    "            f\"top 5 lags are : {lags_values_str}<|<end_prompt>|>\"\n",
    "        )\n",
    "        prompt.append(prompt_)\n",
    "    return prompt\n",
    "\n",
    "def tokenize_prompt_and_get_prompt_embeddings(prompt, tokenizer, llm_model, device):\n",
    "    prompt = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).input_ids\n",
    "    prompt_embeddings = llm_model.get_input_embeddings()(prompt.to(device))  # (batch, prompt_token, dim)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T22:02:29.284913Z",
     "start_time": "2024-10-10T22:02:29.271337Z"
    }
   },
   "id": "f3e0780482b6aee8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "we can now update our model with the prompt generation and tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cdb84b230c493c6"
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from llama import Llama\n",
    "from layers.StandardNorm import Normalize\n",
    "from Embeddings.patch_embedding import PatchEmbedding\n",
    "\n",
    "def generate_prompt(x_enc, description, pred_len, seq_len):\n",
    "    def calcute_lags(_x_enc):\n",
    "        q_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)\n",
    "        k_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)\n",
    "        res = q_fft * torch.conj(k_fft)\n",
    "        corr = torch.fft.irfft(res, dim=-1)\n",
    "        mean_value = torch.mean(corr, dim=1)\n",
    "        _, lags = torch.topk(mean_value, 5, dim=-1)\n",
    "        return lags\n",
    "    \n",
    "    B, T, N = x_enc.size()\n",
    "    x_enc = x_enc.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n",
    "    min_values = torch.min(x_enc, dim=1)[0]\n",
    "    max_values = torch.max(x_enc, dim=1)[0]\n",
    "    medians = torch.median(x_enc, dim=1).values\n",
    "    lags = calcute_lags(x_enc)\n",
    "    trends = x_enc.diff(dim=1).sum(dim=1)\n",
    "    \n",
    "    prompt = []\n",
    "    for b in range(x_enc.shape[0]):\n",
    "        min_values_str = str(min_values[b].tolist()[0])\n",
    "        max_values_str = str(max_values[b].tolist()[0])\n",
    "        median_values_str = str(medians[b].tolist()[0])\n",
    "        lags_values_str = str(lags[b].tolist())\n",
    "        prompt_ = (\n",
    "            f\"<|start_prompt|>Dataset description: {description}\"\n",
    "            f\"Task description: forecast the next {str(pred_len)} steps given the previous {str(seq_len)} steps information; \"\n",
    "            \"Input statistics: \"\n",
    "            f\"min value {min_values_str}, \"\n",
    "            f\"max value {max_values_str}, \"\n",
    "            f\"median value {median_values_str}, \"\n",
    "            f\"the trend of input is {'upward' if trends[b] > 0 else 'downward'}, \"\n",
    "            f\"top 5 lags are : {lags_values_str}<|<end_prompt>|>\"\n",
    "        )\n",
    "        prompt.append(prompt_)\n",
    "    return prompt\n",
    "\n",
    "def tokenize_prompt_and_get_prompt_embeddings(prompt, tokenizer, llm_model, device):\n",
    "    prompt = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).input_ids\n",
    "    prompt_embeddings = llm_model.get_input_embeddings()(prompt.to(device))  # (batch, prompt_token, dim)\n",
    "    return prompt_embeddings\n",
    "\n",
    "class MappingLayer(nn.Module):\n",
    "    def __init__(self, word_embeddings, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.vocab_size = word_embeddings.shape[0]\n",
    "        self.num_tokens = 1000\n",
    "        self.mapping_layer = nn.Linear(self.vocab_size, self.num_tokens)\n",
    "    def forward(self, word_embeddings):\n",
    "        source_embeddings = self.mapping_layer(word_embeddings)\n",
    "        return source_embeddings\n",
    "        \n",
    "\n",
    "class TlModel(nn.Module):\n",
    "    def __init__(self, configs, patch_len=16, stride=8):\n",
    "        super(TlModel, self).__init__()\n",
    "        self.task_name = \"long_term_forecast\"\n",
    "        self.pred_len = 96\n",
    "        self.seq_len = 512\n",
    "        self.d_ff = 128\n",
    "        self.top_k = 5\n",
    "        self.d_llm = 4096\n",
    "        self.patch_len = 16\n",
    "        self.stride = 8\n",
    "        \n",
    "        self.llama = Llama()\n",
    "        \n",
    "        # taken as it is from the paper\n",
    "        # configs.enc_in = 7\n",
    "        self.normalize_layers = Normalize(7, affine=False)\n",
    "        self.mapping_layer = MappingLayer(self.llama.get_model().get_input_embeddings().weight)\n",
    "        \n",
    "        self.word_embeddings = self.llama.get_model().get_input_embeddings().weight\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(32, self.patch_len, self.stride, 0.1)\n",
    "        \n",
    "        self.description = \"The Electricity Transformer Temperature (ETT) is a crucial indicator in the electric power long-term deployment.\"\n",
    "    \n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n",
    "        dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
    "        return dec_out\n",
    "        #return dec_out[:, -self.pred_len:, :]\n",
    "    \n",
    "    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "            x_enc = self.normalize_layers(x_enc, 'norm')\n",
    "            \n",
    "            # Generate prompt and get embeddings\n",
    "            prompt = generate_prompt(x_enc, self.description, self.pred_len, self.seq_len)\n",
    "            tokenizer= self.llama.get_tokenizer()\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "            prompt_embeddings = tokenize_prompt_and_get_prompt_embeddings(prompt, \n",
    "                                                                          self.llama.get_tokenizer(), \n",
    "                                                                          self.llama.get_model(), \n",
    "                                                                          x_enc.device)\n",
    "            print(f\"prompt embedding shape {prompt_embeddings.shape}\")\n",
    "            print(f\"prompt embedding {prompt_embeddings.shape}\")\n",
    "            \n",
    "            # Get source embeddings \n",
    "            word_embeddings = self.llama.get_model().get_input_embeddings().weight\n",
    "            source_embeddings = self.mapping_layer(word_embeddings.permute(1, 0)).permute(1, 0)    \n",
    "            print(f\"source embeddings shape {source_embeddings.shape}\")\n",
    "            \n",
    "            # Get patch embeddings\n",
    "            x_enc = x_enc.permute(0, 2, 1).contiguous()\n",
    "            x_enc = x_enc.to(torch.bfloat16)\n",
    "            enc_out, n_vars = self.patch_embedding(x_enc.to(torch.bfloat16))\n",
    "            \n",
    "            print(f\"patch embeddings shape {enc_out.shape}\")\n",
    "            #enc_out = reprograming_layer(source_embeddings, x_enc, prompt_embeddings)\n",
    "            #llama_enc_out = torch.cat([prompt_embeddings, enc_out], dim=1)\n",
    "            \n",
    "            \n",
    "            return x_enc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T22:02:29.297083Z",
     "start_time": "2024-10-10T22:02:29.294550Z"
    }
   },
   "id": "d0abed6ad91cd6df"
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "377110ff602a4f97978da51e4521e5ed"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 384, 1])\n",
      "prompt embedding shape torch.Size([24, 175, 4096])\n",
      "prompt embedding torch.Size([24, 175, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:08, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source embeddings shape torch.Size([1000, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type BFloat16 but found Float",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[90], line 14\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m#print(torch.reshape(batch_x[0], (1, 384)))\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(batch_x\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m---> 14\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch_x\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_x_mark\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_y\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_y_mark\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m#print(torch.reshape(output[0], (1, 384)))\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(output\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[0;32m~/Documents/Workspace/Learning/retimellm/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Workspace/Learning/retimellm/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[89], line 88\u001B[0m, in \u001B[0;36mTlModel.forward\u001B[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask)\u001B[0m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x_enc, x_mark_enc, x_dec, x_mark_dec, mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m---> 88\u001B[0m     dec_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforecast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_enc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_mark_enc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_dec\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_mark_dec\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     89\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m dec_out\n",
      "Cell \u001B[0;32mIn[89], line 115\u001B[0m, in \u001B[0;36mTlModel.forecast\u001B[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec)\u001B[0m\n\u001B[1;32m    113\u001B[0m x_enc \u001B[38;5;241m=\u001B[39m x_enc\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[1;32m    114\u001B[0m x_enc \u001B[38;5;241m=\u001B[39m x_enc\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mbfloat16)\n\u001B[0;32m--> 115\u001B[0m enc_out, n_vars \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpatch_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_enc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbfloat16\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpatch embeddings shape \u001B[39m\u001B[38;5;132;01m{\u001B[39;00menc_out\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    118\u001B[0m \u001B[38;5;66;03m#enc_out = reprograming_layer(source_embeddings, x_enc, prompt_embeddings)\u001B[39;00m\n\u001B[1;32m    119\u001B[0m \u001B[38;5;66;03m#llama_enc_out = torch.cat([prompt_embeddings, enc_out], dim=1)\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Workspace/Learning/retimellm/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Workspace/Learning/retimellm/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/research/retimellm/Embeddings/patch_embedding.py:56\u001B[0m, in \u001B[0;36mPatchEmbedding.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     54\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mreshape(x, (x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m*\u001B[39m x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m], x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m3\u001B[39m]))\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# Input encoding\u001B[39;00m\n\u001B[0;32m---> 56\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalue_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(x), n_vars\n",
      "File \u001B[0;32m~/Documents/Workspace/Learning/retimellm/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Workspace/Learning/retimellm/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/research/retimellm/Embeddings/patch_embedding.py:18\u001B[0m, in \u001B[0;36mTokenEmbedding.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 18\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenConv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpermute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/Documents/Workspace/Learning/retimellm/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Workspace/Learning/retimellm/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/Workspace/Learning/retimellm/lib/python3.9/site-packages/torch/nn/modules/conv.py:308\u001B[0m, in \u001B[0;36mConv1d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 308\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/Workspace/Learning/retimellm/lib/python3.9/site-packages/torch/nn/modules/conv.py:301\u001B[0m, in \u001B[0;36mConv1d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    299\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_conv_forward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor, weight: Tensor, bias: Optional[Tensor]):\n\u001B[1;32m    300\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m--> 301\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reversed_padding_repeated_twice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_mode\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    302\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    303\u001B[0m \u001B[43m                        \u001B[49m\u001B[43m_single\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv1d(\u001B[38;5;28minput\u001B[39m, weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    305\u001B[0m                     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: expected scalar type BFloat16 but found Float"
     ]
    }
   ],
   "source": [
    "dataset = Dataset_ETT_hour()\n",
    "data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=24,\n",
    "        shuffle=False,\n",
    "        num_workers=1,\n",
    "        drop_last=True\n",
    "    )\n",
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in tqdm(enumerate(data_loader)):\n",
    "        print(f\"batch {i}\")\n",
    "        model = TlModel(configs=None)\n",
    "        #print(torch.reshape(batch_x[0], (1, 384)))\n",
    "        print(batch_x.shape)\n",
    "        output = model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n",
    "        #print(torch.reshape(output[0], (1, 384)))\n",
    "        print(output.shape)\n",
    "        if i == 0:\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-10T22:02:43.605302Z",
     "start_time": "2024-10-10T22:02:29.298206Z"
    }
   },
   "id": "8b38c9288437cf52"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-10-10T22:02:43.602747Z"
    }
   },
   "id": "7675f623a7fd12f4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
